"""
dataset.py: Dataset preparation for GAN decoder training using CFM Distillation.

Fixed version - Uses correct synthesise() method instead of accessing cfm directly.

This version integrates with the existing TokAN data pipeline and follows
the same split strategy as teacher_target_datamodule.py.

Split Strategy:
1. Test speakers (EBVS, SKA) → completely held out for test_speaker evaluation
2. Training speakers (22 L2 speakers):
   - 50 audio files → test_sentence split (unseen sentences)
   - 50 audio files → validation split
   - Remaining → training split

Training Data:
- Input: h_y from frozen encoder (with L2 speaker embedding)
- Target: Mel spectrogram generated by CFM decoder (native accent + L2 voice)
"""

import json
import logging
from pathlib import Path
from typing import Dict, List, Optional
from collections import defaultdict

import numpy as np
import torch
import torch.nn.functional as F
import torchaudio
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm

logger = logging.getLogger(__name__)

# =============================================================================
# Speaker Configuration
# =============================================================================

# Test speakers - completely held out for evaluation
TEST_SPEAKERS = ["EBVS", "SKA"]

# All L2 speakers by accent (24 total)
L2_SPEAKERS = {
    "arabic": ["ABA", "SKA", "YBAA", "ZHAA"],
    "chinese": ["BWC", "LXC", "NCC", "TXHC"],
    "hindi": ["ASI", "RRBI", "SVBI", "TNI"],
    "korean": ["HJK", "HKK", "YDCK", "YKWK"],
    "spanish": ["EBVS", "ERMS", "MBMPS", "NJS"],
    "vietnamese": ["HQTV", "PNV", "THV", "TLV"],
    "us": ["BDL", "SLT", "CLB", "RMS"]
}

# Training speakers (26 = 28 - 2 test speakers)
TRAIN_SPEAKERS = [
    spk for speakers in L2_SPEAKERS.values() 
    for spk in speakers if spk not in TEST_SPEAKERS
]

# Per-speaker split configuration
DEFAULT_SENTENCES_PER_SPEAKER_TEST = 50
DEFAULT_SENTENCES_PER_SPEAKER_VAL = 50


# =============================================================================
# Mel Spectrogram Extractor
# =============================================================================

class MelExtractor:
    """Extract mel spectrograms from audio."""
    
    def __init__(
        self,
        sample_rate: int = 22050,
        n_fft: int = 1024,
        hop_length: int = 256,
        win_length: int = 1024,
        n_mels: int = 80,
        f_min: float = 0.0,
        f_max: float = 8000.0,
    ):
        self.sample_rate = sample_rate
        self.hop_length = hop_length
        
        self.mel_transform = torchaudio.transforms.MelSpectrogram(
            sample_rate=sample_rate,
            n_fft=n_fft,
            hop_length=hop_length,
            win_length=win_length,
            n_mels=n_mels,
            f_min=f_min,
            f_max=f_max,
            power=1,
        )

    def __call__(self, wav: torch.Tensor) -> torch.Tensor:
        if wav.dim() == 1:
            wav = wav.unsqueeze(0)
        mel = self.mel_transform(wav)
        mel = torch.log(torch.clamp(mel, min=1e-5))
        return mel.squeeze(0)


# =============================================================================
# TokAN Full Pipeline Extractor (for CFM Distillation)
# =============================================================================

class TokANDistillationExtractor:
    """
    Extract training pairs using the full TokAN pipeline.
    
    Extracts both h_y (encoder output) and mel (CFM decoder output)
    to enable knowledge distillation from CFM to GAN decoder.
    """
    
    def __init__(
        self,
        checkpoint_path: str,
        device: torch.device,
        n_timesteps: int = 10,
    ):
        self.device = device
        self.checkpoint_path = checkpoint_path
        self.n_timesteps = n_timesteps
        self._model = None

    def setup(self):
        """Load the pretrained model."""
        from tokan.yirga.models.yirga_token_to_mel import YirgaTokenToMel
        
        logger.info(f"Loading TokAN model from {self.checkpoint_path}")
        self._model = YirgaTokenToMel.load_from_checkpoint(
            self.checkpoint_path,
            map_location=self.device
        ).to(self.device)
        self._model.eval()
        
        for param in self._model.parameters():
            param.requires_grad = False
        
        logger.info(f"Model loaded. Using {self.n_timesteps} CFM timesteps.")

    @property
    def model(self):
        if self._model is None:
            self.setup()
        return self._model

    @torch.inference_mode()
    def extract_training_pair(
        self,
        x: torch.Tensor,
        x_lengths: torch.Tensor,
        spks: torch.Tensor,
    ) -> Dict[str, torch.Tensor]:
        """
        Extract h_y and CFM-generated mel for training.
        
        Args:
            x: Token indices (B, T_x)
            x_lengths: Token lengths (B,)
            spks: Speaker embeddings (B, spk_dim)
            
        Returns:
            Dictionary with h_y, mel_target, y_mask, y_lengths, spk_embed
        """
        model = self.model
        
        # Encode tokens
        x_enc, x_mask = model.encoder(x, x_lengths, spks)
        
        # Add speaker embedding
        spk_embed = model.spk_embedder(spks)
        x_enc = x_enc + spk_embed.unsqueeze(-1) * x_mask
        
        # Compute durations and alignment
        if model.predict_duration:
            w = model.duration_predictor(x_enc, x_mask)
            w_ceil = model.duration_predictor.round_duration(w)
            y_lengths = torch.clamp_min(torch.sum(w_ceil, [1, 2]), 1).long()
            
            from tokan.matcha.utils.model import generate_path, sequence_mask, fix_len_compatibility
            
            y_max_length = y_lengths.max()
            y_max_length_ = fix_len_compatibility(y_max_length)
            y_mask = sequence_mask(y_lengths, y_max_length_).unsqueeze(1).to(x_mask.dtype)
            attn_mask = x_mask.unsqueeze(-1) * y_mask.unsqueeze(2)
            attn = generate_path(w_ceil.squeeze(1), attn_mask.squeeze(1)).unsqueeze(1)
        else:
            from tokan.yirga.utils.model import generate_even_path
            from tokan.matcha.utils.model import sequence_mask
            
            y_lengths = torch.floor(x_lengths * model.upsample_rate).long()
            y_max_length = y_lengths.max()
            y_mask = sequence_mask(y_lengths, y_max_length).unsqueeze(1).to(x_mask.dtype)
            attn_mask = x_mask.unsqueeze(-1) * y_mask.unsqueeze(2)
            attn = generate_even_path(model.upsample_rate, attn_mask.squeeze(1))
        
        # Expand encoder output to get h_y
        h_y = torch.matmul(attn.squeeze(1).transpose(1, 2), x_enc.transpose(1, 2))
        h_y = h_y.transpose(1, 2)
        
        # Update mask
        from tokan.matcha.utils.model import sequence_mask
        y_max_length = y_lengths.max()
        y_mask = sequence_mask(y_lengths, y_max_length).unsqueeze(1).to(h_y.dtype)
        
        # === FIX: Use synthesise() method instead of model.cfm.inference() ===
        # Generate mel using the model's synthesise method
        output = model.synthesise(
            x=x,
            x_lengths=x_lengths,
            spks=spks,
            cond=None,
            total_duration=None,
            n_timesteps=self.n_timesteps
        )
        mel_target = output["mel"]
        
        # Ensure mel and h_y have matching time dimensions
        # Since synthesise might produce slightly different lengths, we need to align them
        h_y_len = h_y.shape[-1]
        mel_len = mel_target.shape[-1]
        
        if mel_len > h_y_len:
            mel_target = mel_target[..., :h_y_len]
            y_lengths = torch.tensor([h_y_len], device=h_y.device)
            y_mask = sequence_mask(y_lengths, h_y_len).unsqueeze(1).to(h_y.dtype)
        elif h_y_len > mel_len:
            h_y = h_y[..., :mel_len]
            y_lengths = torch.tensor([mel_len], device=h_y.device)
            y_mask = sequence_mask(y_lengths, mel_len).unsqueeze(1).to(h_y.dtype)
        
        return {
            "h_y": h_y,
            "mel_target": mel_target,
            "y_mask": y_mask,
            "y_lengths": y_lengths,
            "spk_embed": spks,
        }


# =============================================================================
# Preprocessing Function
# =============================================================================

def preprocess_gan_targets(
    data_dir: str,
    output_dir: str,
    tokan_checkpoint: str,
    hubert_path: str,
    hubert_km_path: str,
    device: str = "cuda",
    cfm_timesteps: int = 10,
    speakers: Optional[List[str]] = None,
):
    """
    Preprocess L2-Arctic data for GAN decoder training using CFM distillation.
    
    Creates directory structure:
        output_dir/
        ├── manifest.json
        ├── h_y/
        │   └── SPEAKER/filename.npy
        ├── mels/
        │   └── SPEAKER/filename.npy
        └── spk_embeds/
            └── SPEAKER/filename.npy
    
    Args:
        data_dir: Path to L2-Arctic data directory
        output_dir: Where to save preprocessed files
        tokan_checkpoint: Path to TokAN model checkpoint
        hubert_path: Path to HuBERT model
        hubert_km_path: Path to k-means model
        device: cuda or cpu
        cfm_timesteps: Number of CFM timesteps for target generation
        speakers: List of speakers to process (None = all L2 speakers)
    """
    from resemblyzer import VoiceEncoder
    
    device = torch.device(device)
    data_path = Path(data_dir)
    output_path = Path(output_dir)
    
    # Create output directories
    (output_path / "h_y").mkdir(parents=True, exist_ok=True)
    (output_path / "mels").mkdir(parents=True, exist_ok=True)
    (output_path / "spk_embeds").mkdir(parents=True, exist_ok=True)
    
    # Determine which speakers to process
    if speakers is None:
        speakers = [spk for spks in L2_SPEAKERS.values() for spk in spks]
    
    # Initialize components
    logger.info("Initializing components...")
    
    speaker_encoder = VoiceEncoder(device=device)
    
    tokan_extractor = TokANDistillationExtractor(
        tokan_checkpoint, device, n_timesteps=cfm_timesteps
    )
    tokan_extractor.setup()
    
    from tokan.textless.hubert_feature_reader import HubertFeatureReader
    from tokan.textless.kmeans_quantizer import KMeansQuantizer
    from tokan.textless.speech_encoder import SpeechEncoder
    
    hubert = HubertFeatureReader(hubert_path, layer=17)
    kmeans = KMeansQuantizer(hubert_km_path)
    speech_encoder = SpeechEncoder(
        hubert, kmeans, need_f0=False, deduplicate=True, padding=True
    ).to(device)
    
    # Get accent for each speaker
    speaker_to_accent = {}
    for accent, spks in L2_SPEAKERS.items():
        for spk in spks:
            speaker_to_accent[spk] = accent
    
    # Collect all audio files
    all_files = []
    for speaker in speakers:
        speaker_dir = data_path / speaker
        if not speaker_dir.exists():
            logger.warning(f"Speaker directory not found: {speaker_dir}")
            continue
        
        wav_dir = speaker_dir / "wav"
        if wav_dir.exists():
            wav_files = list(wav_dir.glob("*.wav"))
        else:
            wav_files = list(speaker_dir.glob("*.wav"))
        
        for wav_file in wav_files:
            all_files.append({
                "wav_path": wav_file,
                "speaker": speaker,
                "accent": speaker_to_accent.get(speaker, "unknown"),
            })
        
        logger.info(f"Found {len(wav_files)} files for speaker {speaker}")
    
    logger.info(f"Total files to process: {len(all_files)}")
    
    # Process files
    manifest = []
    success_count = 0
    fail_count = 0
    
    for file_info in tqdm(all_files, desc="Processing with CFM distillation"):
        wav_path = file_info["wav_path"]
        speaker = file_info["speaker"]
        accent = file_info["accent"]
        
        try:
            # Create output paths
            filename = wav_path.stem
            h_y_rel_path = f"h_y/{speaker}/{filename}.npy"
            mel_rel_path = f"mels/{speaker}/{filename}.npy"
            spk_rel_path = f"spk_embeds/{speaker}/{filename}.npy"
            
            # Create speaker subdirectories
            (output_path / "h_y" / speaker).mkdir(exist_ok=True)
            (output_path / "mels" / speaker).mkdir(exist_ok=True)
            (output_path / "spk_embeds" / speaker).mkdir(exist_ok=True)
            
            # Skip if already processed
            if (output_path / h_y_rel_path).exists():
                # Add to manifest anyway
                manifest.append({
                    "id": f"{speaker}_{filename}",
                    "audio_path": str(wav_path),
                    "h_y_path": h_y_rel_path,
                    "mel_path": mel_rel_path,
                    "spk_embed_path": spk_rel_path,
                    "speaker_id": speaker,
                    "accent": accent,
                })
                success_count += 1
                continue
            
            # Load audio at 16kHz for speech encoding
            from silero_vad import read_audio
            wav_16k = read_audio(str(wav_path), sampling_rate=16000)
            
            # Extract speaker embedding
            spk_embed = speaker_encoder.embed_utterance(wav_16k.numpy())
            spk_embed = torch.FloatTensor(spk_embed)
            
            # Extract content tokens
            with torch.inference_mode():
                tokens = speech_encoder(wav_16k.to(device))["units"].squeeze(0)
            
            # Run through TokAN pipeline
            with torch.inference_mode():
                x = tokens.unsqueeze(0).to(device)
                x_lengths = torch.tensor([len(tokens)]).to(device)
                spks = spk_embed.unsqueeze(0).to(device)
                
                outputs = tokan_extractor.extract_training_pair(x, x_lengths, spks)
                
                h_y = outputs["h_y"].squeeze(0).cpu().numpy()
                mel_target = outputs["mel_target"].squeeze(0).cpu().numpy()
            
            # Save files
            np.save(output_path / h_y_rel_path, h_y)
            np.save(output_path / mel_rel_path, mel_target)
            np.save(output_path / spk_rel_path, spk_embed.numpy())
            
            # Add to manifest
            manifest.append({
                "id": f"{speaker}_{filename}",
                "audio_path": str(wav_path),
                "h_y_path": h_y_rel_path,
                "mel_path": mel_rel_path,
                "spk_embed_path": spk_rel_path,
                "speaker_id": speaker,
                "accent": accent,
            })
            
            success_count += 1
            
        except Exception as e:
            logger.warning(f"Failed to process {wav_path}: {e}")
            fail_count += 1
            continue
    
    # Save manifest
    manifest_path = output_path / "manifest.json"
    with open(manifest_path, "w") as f:
        json.dump(manifest, f, indent=2)
    
    logger.info(f"\nPreprocessing complete!")
    logger.info(f"  Success: {success_count}")
    logger.info(f"  Failed: {fail_count}")
    logger.info(f"  Manifest: {manifest_path}")
    
    # Print summary by speaker
    logger.info("\nSamples by speaker:")
    by_speaker = defaultdict(int)
    for sample in manifest:
        by_speaker[sample["speaker_id"]] += 1
    for speaker, count in sorted(by_speaker.items()):
        marker = " [TEST]" if speaker in TEST_SPEAKERS else ""
        logger.info(f"  {speaker}: {count}{marker}")


# =============================================================================
# Main Entry Point
# =============================================================================

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Preprocess L2-Arctic for GAN decoder")
    parser.add_argument("--data_dir", required=True,
                        help="Path to L2-Arctic data directory")
    parser.add_argument("--output_dir", required=True,
                        help="Output directory for preprocessed files")
    parser.add_argument("--tokan_checkpoint", required=True,
                        help="Path to TokAN model checkpoint")
    parser.add_argument("--hubert_path", required=True,
                        help="Path to HuBERT model")
    parser.add_argument("--hubert_km_path", required=True,
                        help="Path to k-means model")
    parser.add_argument("--device", default="cuda")
    parser.add_argument("--cfm_timesteps", type=int, default=10,
                        help="CFM timesteps for target generation")
    parser.add_argument("--speakers", nargs="+", default=None,
                        help="Specific speakers to process (default: all L2)")
    args = parser.parse_args()
    
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s"
    )
    
    preprocess_gan_targets(
        data_dir=args.data_dir,
        output_dir=args.output_dir,
        tokan_checkpoint=args.tokan_checkpoint,
        hubert_path=args.hubert_path,
        hubert_km_path=args.hubert_km_path,
        device=args.device,
        cfm_timesteps=args.cfm_timesteps,
        speakers=args.speakers,
    )